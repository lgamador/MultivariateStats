---
title: "Lab 2 Multivariate Data Screening"
output:
  word_document: default
  pdf_document: default
  html_document:
    number_sections: yes
---

```{r setup, include=FALSE}
library(knitr)
opts_knit$set(root.dir = normalizePath("../"))
opts_chunk$set(echo = TRUE, tidy = TRUE)
```

# Set up R session

## Install and load packages.

Note that for a RMarkdown document to knit, you need to comment out these 'install.packages' lines of code before knitting. This is because you cannot knit when your document needs to connect to an external web server. Right now these lines of code are commented out.

```{r eval=FALSE, warning=FALSE, message=FALSE}
# install.packages("mvnormtest")
# install.packages("MVN")
# install.packages("MVA")
# install.packages("psych")
# install.packages("Hmisc")
# install.packages("vegan")
# install.packages("StatMatch")
# install.packages("MASS")
# install.packages("raster")
# install.packages("cluster")
```

```{r warning=FALSE, message=FALSE}
library(mvnormtest)
library(MVN)
library(MVA)
library(psych)
library(Hmisc)
library(vegan)
library(StatMatch)
library(MASS)
library(raster)
library(cluster)
```


## Importing Data

We will be using the `USairpollution` and the `usAir_mod.csv` data sets for these examples.

The MVA US air pollution data set:

```{r}
usAir<-USairpollution
```

The modified `USairpollution` data set from your working directory is a csv file. Note you will need to modify you working directory if it is different.
```{r}
usAir_mod<- read.csv("G:/Shared drives/MultivariateStatistics/Data/LabData/Lab2/usAir_mod.csv", row=1, header=TRUE)
```

# Data screening

Your first move when conducting a multivariate analysis (or any analysis) is to screen the data. You are looking for data errors, missing data, and outliers that may influence your analysis.

## Data errors

One way to check for data errors is to examine the summary statistics for your data set.

First look at the summary statistics for `usAir`:

```{r eval=FALSE}
describeBy(usAir)
```

***Question 1: Do you see any unrealistic values? (5 pts) Note please answer all questions with points related to them.***\ 
The standard deviation values for `manu` and `popul` as they are much larger that the mean and median values, indicating some large spread. The range of values for these two also seem pretty extreme so that brings some suspicion. Wind values also seem to be pretty low but also given the context of the cities, Phoenix has the lowest wind value which makes sense since it is generally flat and there are not many tall buildings compared to Chicago with a much higher wind value.\  

Now look at the summary statistics for `usAir_mod`:
```{r eval=FALSE}
describeBy(usAir_mod)
```

Look at the max for temperature. It will be easier to look for data errors when it is your own data.

## Missing Data

When you have missing entries in your data sheet, R replaces them with “NA”. You can check if you have any missing variables in *usAir_mod*:
```{r eval=FALSE}
describe(usAir_mod)
```

The *describe* function provides some of the same information as *describeBy*, but importantly shows you which variables have missing values.

We talked about two methods for dealing with missing values in lecture; **Complete Case and Imputation**. We will look at **complete case and imputation** for now.

**Complete Case** involves the removal of samples (in this case cities) with missing data:
```{r eval=FALSE}
usAir_mod [complete.cases(usAir_mod),]
```

**Imputation** involves filling in missing values with plausible data. Let’s replace NAs with the mean of the variable.
```{r}
#First, let’s calculate the mean of each variable (column) with the NA removed:

meanz<-colMeans(usAir_mod,na.rm=T)

#`na.rm=T`, means that you want to remove NAs

#To replace your NAs with the means you just calculated you will use the following function:

naFunc<-function(column) { 
  column[is.na(column)]  = round(mean(column, na.rm = TRUE),2)
  return(column) 
}

#and “apply” it to the usair_mod data set

Impute<-apply(usAir_mod,2,naFunc)

```

Check out the new Impute data object and make sure that the NA’s have been replaced.

We will not go into this advanced function too much. However, know that *apply* allows us to perform a function on all the rows and/or columns in a data frame of matrix. As we spoke about in lecture, there are many types of imputation methods. We can explore further methods for your specific missing data.

# Multivariate Normal Distribution

Many of the analyses we will do in this course have an assumption of multivariate normality. While there are many tests of multivariate normality, they tend to be overly conservative. If we strictly followed these tests, we may never run a multivariate analysis with ecological or agricultural data. Here we will look at two multivariate tests of normality.

## Shapiro-Wilks test
Shapiro-Wilks tests if the distribution of the observed data differs from multivariate normal distribution. So, we are looking for p-values \> 0.05.
```{r}
mshapiro.test(t(usAir))
```

## Mardia test
Mardia’s test looks at multivariate extensions of Skewness and Kurtosis. In both cases, we are looking for p-values \> 0.05 to show that our data do not deviate from the expectations of multivariate normal Skewness and Kurtosis. For the observed data to be considered multivariate normal, p-values from both the Skewness and Kurtosis statistics must be \> 0.05. This function also tests for univariate normality of residuals using the Shapiro-Wilk statistic.
```{r}
mvn(usAir, mvnTest = "mardia")
```

# Data transformation

The next step is preparing your data for analysis is transforming the data. Today we will look at the log, square root, and arcsine square root transformations.

## Log transformation: $$b_{ij}=log(x_{ij}+1)$$

Several common transformations have built-in functions in R. While you can build transformation functions on your own, we will use the ones R has developed today. First, let’s look at a histogram of our first variable, SO2, to determine if transformation is necessary:

Remember, to extract the SO2 column:

```{r eval=FALSE}
usAir$SO2 

#or 

usAir[,1] 


#Next you can simply wrap either of those commands in the histogram function:

hist(usAir$SO2) 

#or 

hist(usAir[,1])  
```

To log transform each value in our data frame:

```{r results='hide'}
usAirlog<-log1p(usAir)

```

and the histogram:

```{r eval=FALSE}
hist(usAirlog$SO2) 

#or 

hist(usAirlog[,1])
```

You can compare the histograms side by side using the par function followed by hist:

```{r}
par(mfrow=c(1,2))
hist(usAir[,1])  
hist(usAirlog[,1])  
```

Placing 1, 2 in parentheses after the `c` (which stands for concatenate) in the `par` function indicates that you want you plots arranged in 1 row and two columns. Note this plotting is done in base R as opposed to using the ggplot functions of Tidyverse. It is helpful to know base R and Tidyverse to be able to read and trouble shoot code with a wide range of collaborators. In ggplot this code would be similar to what the *facet* function does.

Compare histograms for the raw data and the log transformed data for each variable.
```{r}
par(mfrow=c(2,4))
#SO2
hist(usAir$SO2, main = "raw SO2", xlab="")
hist(usAirlog$SO2, main = "log SO2", xlab="")

#temp
hist(usAir$temp, main = "raw temp", xlab="")
hist(usAirlog$temp, main = "log temp", xlab="")

#manu
hist(usAir$manu, main = "raw manu", xlab="")
hist(usAirlog$manu, main = "log manu", xlab="")

#popul
hist(usAir$popul, main = "raw popul", xlab="")
hist(usAirlog$popul, main = "log popul", xlab="")

#wind
hist(usAir$wind, main = "raw wind", xlab="")
hist(usAirlog$wind, main = "log wind", xlab="")

#precip
hist(usAir$precip, main = "raw precip", xlab="")
hist(usAirlog$precip, main = "log precip", xlab="")

#predays
hist(usAir$predays, main = "raw predays", xlab="")
hist(usAirlog$predays, main = "log predays", xlab="")
```


**Question 2: Which variable might not need to be log transformed? (5 pts)**
The `wind` and `predays` seem to not need log transformation since they appear to be normal. Temp and precip may also not need log but it seems they are slightly improved. 
\newline

## Square root transformation: $$b_{ij}=\sqrt{x_{ij}}$$

To square root transform each value in our data frame:

```{r results='hide'}
usAirsqrt<-sqrt(usAir)
```

and the histogram:

```{r eval=FALSE}
hist(usAirsqrt$SO2)

#or 

hist(usAirsqrt[,1])  
```

Compare the histograms side by side using the par function followed by hist:

```{r}
par(mfrow=c(1,2))

hist(usAir[,1])  
hist(usAirsqrt[,1]) 
```

Compare histograms for the raw data and the square root transformed data for each variable…
```{r}
par(mfrow=c(2,4))
#SO2
hist(usAir$SO2, main = "raw SO2", xlab="")
hist(usAirsqrt$SO2, main = "sqrt SO2", xlab="")

#temp
hist(usAir$temp, main = "raw temp", xlab="")
hist(usAirsqrt$temp, main = "sqrt temp", xlab="")

#manu
hist(usAir$manu, main = "raw manu", xlab="")
hist(usAirsqrt$manu, main = "sqrt manu", xlab="")

#popul
hist(usAir$popul, main = "raw popul", xlab="")
hist(usAirsqrt$popul, main = "sqrt popul", xlab="")

#wind
hist(usAir$wind, main = "raw wind", xlab="")
hist(usAirsqrt$wind, main = "sqrt wind", xlab="")

#precip
hist(usAir$precip, main = "raw precip", xlab="")
hist(usAirsqrt$precip, main = "sqrt precip", xlab="")

#predays
hist(usAir$predays, main = "raw predays", xlab="")
hist(usAirsqrt$predays, main = "sqrt predays", xlab="")
```


Remember that square root transformations are best used on count data.

\newline

## Arcsine square root transformation: $$b_{ij}$$ = arcsine$$\sqrt{x_{ij}}$$

If you remember arcsine square root transformations are for percentage data. So, the values for your variable must range between 0 and 1. None of the variables in `usAir` are appropriate for this transformation. Let’s draw some random numbers between 0 and 1 so we can use the arcsine square root transformation.

```{r}
newData<- runif(100, 0, 1)
```

You just chose 100 random values between 0 and 1. Now let’s transform:

```{r eval=FALSE}
asin(sqrt(newData))
```

and compare histograms:

```{r}
par(mfrow=c(1,2))

hist(newData)
hist(asin(sqrt(newData)))
```

# Data standardization

Column standardization adjusts for differences among variables. The focus is on the profile across a sample unit. Row standardization adjusts for differences among sample units, wherein the focus is on the profile within a sample unit. Row standardization is good when variables are measured in the same units (e.g. species). You will more often than not be using column standardization.

## Coefficient of Variation (cv)

Let’s first see if the air pollution data set needs standardization by calculating the *coefficient of variation* **(cv)** for column totals. Remember, the **cv** is the ratio of the standard deviation to the mean (σ/μ):

First calculate the column **sums**:

```{r results='hide'}
cSums<-colSums(usAir)
```

Then calculate the **standard deviation** and **mean** for the column sums:

```{r results='hide'}
Sdev<-sd(cSums)
M<-mean(cSums)
```

Finally, calculate the **cv**:

```{r}
Cv<-Sdev/M*100
```

Our rule of thumb for cv is that if **cv\> 50**, data standardization is necessary.

***Questin 3: Is standardization necessary for the `USairpollution` data? (5 pts)***
Standardization is necessary for the the `USairpollution` dataset (cv = 129.3).\ 

## Z- standardization $$b_{ij}$$ = ($$x_{ij}- \bar{x_{j}})/s_{j}$$

Your goal here is to equalize the variance for variables measured on different scales. There is a built-in function `scale` that will do this for you:

```{r results='hide'}
scaledData<-scale(usAir)
```

Let’s look at histograms for the scaled and unscaled data for the first variable, SO2:

```{r}
par(mfrow=c(1,2))

hist(usAir[,1] ,main=colnames(usAir)[1],xlab=" ")
hist(scaledData[,1] ,main=colnames(usAir)[1],xlab=" ")
```

Compare the raw and standardized histograms for all of the variables.
```{r}
par(mfrow=c(2,4))
#SO2
hist(usAir$SO2, main = "raw SO2", xlab="")
hist(scaledData[,1], main = "Standardized SO2", xlab="")

#temp
hist(usAir$temp, main = "raw temp", xlab="")
hist(scaledData[,2], main = "standardized temp", xlab="")

#manu
hist(usAir$manu, main = "raw manu", xlab="")
hist(scaledData[,3], main = "standardized manu", xlab="")

#popul
hist(usAir$popul, main = "raw popul", xlab="")
hist(scaledData[,4], main = "standardized popul", xlab="")

#wind
hist(usAir$wind, main = "raw wind", xlab="")
hist(scaledData[,5], main = "standardized wind", xlab="")

#precip
hist(usAir$precip, main = "raw precip", xlab="")
hist(scaledData[,6], main = "standardized precip", xlab="")

#predays
hist(usAir$predays, main = "raw predays", xlab="")
hist(scaledData[,7], main = "standardized predays", xlab="")
```

***Question 4: Are you convinced that the variances are equalized? Just to check, calculate the mean and variance for each of the standardized variables. (10 pts)***  
Based off of the calculations the standard deviations seem pretty equalized despite the apparent skewness of the histograms.
```{r}
matmean = colMeans(scaledData) 
matsd = c(sd(scaledData[1,]), sd(scaledData[2,]), sd(scaledData[3,]), sd(scaledData[4,]), sd(scaledData[5,]), sd(scaledData[6,]), sd(scaledData[7,]))
scaledSprd = rbind(matmean, matsd)
rownames(scaledSprd) <- c("mean", "sd")
scaledSprd
```

**Z standardization is very common in life sciences.**

\newline

# Detecting Outliers

Outliers are recorded values of measurements or observations that are outside the range of the bulk of the data. Outliers can inflate variance and lead to erroneous conclusions.

\newline

## Univariate outliers

One way to deal with outliers in multivariate data is to examine each variable separately. You will standardize your data into standard deviation units (z –standardization) and look for values that fall outside of three standard deviations.

First the z-standardization:

```{r results='hide'}
scaledData<-scale(usAir)
```

Next we will create histograms to look for values \> than 3 sd. However, this time we will use the *par fu*nction to look at all seven histograms at once.

```{r eval=FALSE}
par(mfrow=c(2,4))
hist(scaledData [,1] ,main=colnames(usAir)[1],xlab=" ")
hist(scaledData [,2] ,main=colnames(usAir)[2],xlab=" ")  
hist(scaledData [,3] ,main=colnames(usAir)[3],xlab=" ")  
hist(scaledData [,4] ,main=colnames(usAir)[4],xlab=" ")
hist(scaledData [,5] ,main=colnames(usAir)[5],xlab=" ")  
hist(scaledData [,6] ,main=colnames(usAir)[6],xlab=" ")  
hist(scaledData [,7] ,main=colnames(usAir)[7],xlab=" ")
```

Finally, you can identify the outlier(s) for each variable:

```{r eval=FALSE}
scaledData [,1][scaledData [,1]>3] 
scaledData [,2][scaledData [,2]>3]  
scaledData [,3][scaledData [,3]>3] 
scaledData [,4][scaledData [,4]>3]
scaledData [,5][scaledData [,5]>3]
scaledData [,6][scaledData [,6]>3]  
scaledData [,7][scaledData [,7]>3]
```

Alternatively, you could use the apply function, less typing!

For the histogram function (hist):

```{r eval=FALSE}
par(mfrow=c(2,4))
mapply(hist,as.data.frame(usAir),main=colnames(usAir),xlab=" ")
```

Here is a new function for detecting outliers called out.

```{r}
out<-function(x){
lier<-x[abs(x)>3]
return(lier)
}

```

Let’s apply that function:

```{r}
apply(scaledData,2,out)
```

**Question 5: Do you detect any outliers? For which variables? (5 pts)**
Outliers were detected for the following: `SO2`, `temp`, `manu`, `popul`.\ 

## Multivariate outliers

**we will come back to this...**

# Distance and Dissimilarity

As we know from lecture, multivariate data with *p* variables are visually represented by a collection of points forming a data cloud in *p*-dimensional space. The shape, clumping, and dispersion of the data cloud contains information we seek to describe. Several distance and dissimilarity measures are used to calculate the distance between data points.

## Euclidean Distance:

**Euclidean** distance is one of the most commonly used distance measures. It is normally preceded by column standardization (e.g. z standardization). Let's calculate Euclidean distance for the US air pollution data set. You will use the function *`vegdist`* from the *vegan* (vegetation analysis) package. Look up *`vegdist`* to see the different indices available in this package.

```{r eval=FALSE}
?vegdist 
```

First, z standardization:

```{r results='hide'}
scaledData<-scale(usAir)
```

Then calculate distance:

```{r results='hide'}
#Euclidean distance: common distance & for cont data 
eucDist<- vegdist(scaledData,"euclidean")
```

Let’s look at a histogram of distances:

```{r}
hist(eucDist)
```

**Question 6: What does this frequency distribution tell you about pollution conditions across these 41 cities? (5 pts)**
The distance between the majority of the pollution values across cities are close to each other, with a few groups that have much different pollution conditions. \ 

Euclidean Distance can be weird. Let look at the data matrix below:

We want to determine how similar these farms are in the production of strawberries, peaches, and raspberries.

```{r}
Fruit <-rbind(c(1,0,1,1),c(2,1,0,0), c(3,0,4,4))
colnames(Fruit)<-c("Farm","Strawberry","Peach", "Rasberry")
Fruit
```

Calculating Euclidean distance on these data:

```{r}
eucDist<- vegdist(Fruit[,-1], "euclidean")
```

Gives us this distance matrix (R gives you the triangular matrix without the diagonal):
| |1|2|
|2|1.73| |
|3|4.24|5.74|

The distance between farms 1 and 2, which grow none of the same fruits:

$$d_{1,2}=\sqrt{((1-0)^2 )+(1-0)^2+(1-0)^2}=1.732$$

Is **less** (i.e., these farms are more similar in their fruit production) than farms 1 and 3, which grow the same fruit:

$$d_{1,3}=\sqrt{((0-0)^2 )+(1-4)^2+(1-4)^2}=4.234$$

Euclidean distance is not a jack-of-all-trades and is not appropriate for all data sets. Our next distance metric, Manhattan distance would also rank Farms 1 and 2 more similar than 1 and 3.

## City-block (Manhattan) distance

```{r}
cbDist<- vegdist(scaledData,"manhattan")

#Let’s look at a histogram of distances:
hist(cbDist)
```

**Question 7: How does this distribution compare to Euclidean distance? (5 pts)**
The city blok distribution gives a similar shape to the Euclidean distance, but the scales of the distances differ (city block has a larger range).\ 

## Bray-Curtis dissimilarity

```{r}
brayDist<- vegdist(usAir,"bray")

#Histogram:
hist(brayDist)
```

Let’s quickly look at our fruit farm data with Bray-Curtis:

```{r}
brayFruit<- vegdist(Fruit[,-1], "bray")
brayFruit
```

That makes more sense! Farms 1 and 2 (and 2 and 3) are at maximum dissimilarity and farms 1, 3 are more similar.

**Back to multivariate outliers!**

Your goal here is to examine deviations of the sample average distances to other samples. We will use **Bray-Curtis** distance:

```{r}
brayDist<- vegdist(usAir,"bray")
```

Next, calculate column means. These column means represent the average dissimilarity of each city to all other cities. You want to know if any cities are on average more than 3 standard deviation units (z scores). To achieve this, z-transform the averages:

```{r}
multOut<-scale(colMeans(as.matrix(brayDist)))
```

Plot a histogram and look for observations \>3 sd units:
```{r}
hist(multOut)
```

You can find the cities that are outliers with:

```{r}
multOut [multOut >3,]
```

Another possibility is to determine which observation are \> 3 standard deviations from the mean. Using Bray-Curtis distance again:

Calculate column means:

```{r}
colBray<-colMeans(as.matrix(brayDist))
```

Calculate the mean of the column means:

```{r}
mBray<-mean(colBray)
```

Calculate the standard deviation:

```{r}
stdBray<- sd(colBray)
```

… 3 standard deviations

```{r}
threeSD<-stdBray * 3 + mBray
```

plot a histogram and look for observations \>3 sd:

```{r }
hist(colBray)
```

Find the outliers:

```{r}
colBray [colBray >threeSD]
```




# Working through my dataset
**Question 8: NOW, RUN THROUGH THE ABOVE EXERCISES WITH YOUR OWN DATA! (55 pts)**

```{r, reading dataset}
#read in neon-npn phenology dataset with row names appointed to observation IDs
phe = read.csv("G:/Shared drives/MultivariateStatistics/Data/StudentDataSets/AmadorL_NeonNpn_OpenFLowers_conus.CSV", row=1, header = TRUE)

library(tidyverse, quietly = TRUE)
#remove extra columns
phe = phe %>%
  select(-c("update_datetime", "phenophase_id", "phenophase_description", "kingdom", "common_name", "elevation_in_metersStat", "phenophase_status", "intensity_category_id", "intensity_value", "abundance_value", "elevation_in_meters", "first_yes_julian_date", "numdays_since_prior_no", "last_yes_julian_date", "numdays_until_next_no"))
```
Main variables are columns 8, 12, 16, 18 (day_of_year, first_yes_doy, last_yes_doy, mean_first_yes_doy). Each have corresponding years to use. 

## Data screening 
```{r}
describeBy(phe)
describe(phe) #No missing values
```
Day of year values seem to be within a reasonable range. No missing values.

## Multivariate Normal Distribution
All columns must be numerical for `mshapiro.test` and `mvn` function to work, so subsetting in the meantime. Keeping identifier columns, ignore in tests. Look at the normality of variables for the first 5,000 observations
```{r, Shapiro Wilks test}
phe.mvn = phe %>%
  select(c("site_id", "species_id", "individual_id", "day_of_year", "first_yes_doy", "last_yes_doy", "mean_first_yes_doy"))

c = t(phe.mvn[0:5000, 4:7])
mshapiro.test(c) #specifying rows bc of sample size limit & need to specify column 
```
Not normal, p-value < 0.05.


```{r, Mardia test}
mvn(phe.mvn, mvnTest = "mardia")
```
None of the variables are normal, p-value < 0.05.

## Transformations 
### Log transformation on the following: day_of_year, first_yes_doy, last_yes_doy, mean_first_yes_doy
```{r}
phe.log = phe.mvn %>%
  mutate(day_of_year = log(day_of_year), first_yes_doy = log(first_yes_doy), last_yes_doy = log(last_yes_doy), mean_first_yes_doy = log(mean_first_yes_doy))
```

Plot the raw vs log transformed data
```{r}
par(mfrow=c(2,4))
#day of year (DOY)
hist(phe.mvn$day_of_year, main = "raw DOY", xlab = " ")
hist(phe.log$day_of_year, main = "log DOY", xlab = " ")

#first day of year
hist(phe.mvn$first_yes_doy, main = "raw first DOY", xlab = " ")
hist(phe.log$first_yes_doy, main = "log first DOY", xlab = " ")

#last day of year
hist(phe.mvn$last_yes_doy, main = "raw last DOY", xlab = " ")
hist(phe.log$last_yes_doy, main = "log last DOY", xlab = " ")

#average first day of year
hist(phe.mvn$mean_first_yes_doy, main = "raw mean first DOY", xlab = " ")
hist(phe.log$mean_first_yes_doy, main = "log mean first DOY", xlab = " ")

```

### Square root transformation on the following: day_of_year, first_yes_doy, last_yes_doy, mean_first_yes_doy
```{r}
phe.sqrt = phe.mvn %>%
  mutate(day_of_year = sqrt(day_of_year), first_yes_doy = sqrt(first_yes_doy), last_yes_doy = sqrt(last_yes_doy), mean_first_yes_doy = sqrt(mean_first_yes_doy))
```

Plot the raw vs square root transformed data
```{r}
par(mfrow=c(2,4))
#day of year (DOY)
hist(phe.mvn$day_of_year, main = "raw DOY", xlab = " ")
hist(phe.sqrt$day_of_year, main = "sqrt DOY", xlab = " ")

#first day of year
hist(phe.mvn$first_yes_doy, main = "raw first DOY", xlab = " ")
hist(phe.sqrt$first_yes_doy, main = "sqrt first DOY", xlab = " ")

#last day of year
hist(phe.mvn$last_yes_doy, main = "raw last DOY", xlab = " ")
hist(phe.sqrt$last_yes_doy, main = "sqrt last DOY", xlab = " ")

#average first day of year
hist(phe.mvn$mean_first_yes_doy, main = "raw mean first DOY", xlab = " ")
hist(phe.sqrt$mean_first_yes_doy, main = "sqrt mean first DOY", xlab = " ")
```

The square root transformation worked the best. Not performing arcsine square root transformation because my variables are integer/count data (i.e. days into the year until open flower observed).






## Data standardisation  
### Coefficienct of variance
Checking to see if phenology data set needs standardization by calculating the *coefficient of variation* **(cv)** for column totals. 
```{r, results='hide'}
#column sums - only for the four target columns (the first three are identifier info)
cSums <- colSums(phe.mvn[, 4:7])
#stadard deviation
sd <- sd(cSums)
#mean
m <- mean(cSums)
```

```{r}
Cv<-sd/m*100
Cv>50
```

Our rule of thumb for cv is that if **cv\> 50**, data standardization is necessary.
Standardization is not necessary for the phenology data set (cv = 3.54). If it was necessary then we would scale the data using something like Z-standardization (going to do this anyway to detect outliers).

## Detecting outliers
Let's detect univariate outliers. We will Z-standardize the variables (day_of_year, first_yes_doy, last_yes_doy, mean_first_yes_doy) and detect any values outside of 3 standard deviations. 
```{r}
#scaling individually to keep identifier columns
phe.scaled = phe.mvn %>%
  mutate(day_of_year = scale(day_of_year), first_yes_doy = scale(first_yes_doy), last_yes_doy = scale(last_yes_doy), mean_first_yes_doy = scale(mean_first_yes_doy))
```

Let's take a look at the scaled data
```{r}
par(mfrow=c(2,2))

hist(phe.scaled$day_of_year, main = "Day of Year (DOY)", xlab = " ")
hist(phe.scaled$first_yes_doy, main = "First DOY", xlab = " ")
hist(phe.scaled$last_yes_doy, main = "Last DOY", xlab = " ")
hist(phe.scaled$mean_first_yes_doy, main = "Mean first DOY", xlab = " ")
```

Use the `out` function for detecting outliers called out.
```{r}
apply(phe.scaled[,4:7],2,out)
```
Outliers were detected for all variables. The most were found in `first_yes_doy` but all had outliers later in the year. There are plants opening their flowers late in the year.



## Distance & Dissimilarity 

### Euclidean distance
For continuous data, my data is more discrete but let's take a gander. 
First Z-standardizing
```{r results='hide'}
#scaling individually to keep identifier columns
phe.scaled = phe.mvn %>%
  mutate(day_of_year = scale(day_of_year), first_yes_doy = scale(first_yes_doy), last_yes_doy = scale(last_yes_doy), mean_first_yes_doy = scale(mean_first_yes_doy))
```

Then calculate distance & view histogram of distances:
```{r results='hide'}
#Euclidean distance: common distance & for cont data 
eucDist<- vegdist(phe.scaled[4:7],"euclidean")
#histogram of distances
hist(eucDist)
```
The majority of the raw data points are relatively close to each other. 

Note: depending on the data & transformation would need to include the transformed data into the dissimilarty/distance metrics -- especially to identify outliers.  

### City-block (Manhattan) distance
```{r}
cbDist<- vegdist(phe.scaled[4:7],"manhattan")

#Let’s look at a histogram of distances:
hist(cbDist)
```
The city blok distribution gives a similar shape to the Euclidean distance, but the scales of the distances differ (city block has a larger range).

### Bray-Curtis dissimilarity
Let's look at the bray-curtis dissimilarity. In addition, examine deviations of the sample average distances to other samples using **Bray-Curtis** distance.
```{r}
brayDist<- vegdist(phe.mvn[4:7],"bray")

#Histogram:
hist(brayDist)
```

Calculate column means. These column means represent the average dissimilarity of each observation to all other day of year metric. You want to know if any observations are on average more than 3 standard deviation units (z scores). To achieve this, z-transform the averages:
```{r}
multOut<-scale(colMeans(as.matrix(brayDist)))
```

Plot a histogram and look for observations \>3 sd units:
```{r}
hist(multOut)
```
You can find the observations that are outliers with:
```{r}
multOut [multOut >3,]
```


Another possibility is to determine which observation are \> 3 standard deviations from the mean. Using Bray-Curtis distance again:

Calculate column means:
```{r}
colBray<-colMeans(as.matrix(brayDist))
```

Calculate the mean of the column means:
```{r}
mBray<-mean(colBray)
```

Calculate the standard deviation:
```{r}
stdBray<- sd(colBray)
```

… 3 standard deviations
```{r}
threeSD<-stdBray * 3 + mBray
```

plot a histogram and look for observations \>3 sd:
```{r }
hist(colBray)
```

Find the outliers:
```{r}
colBray [colBray >threeSD]
```







