---
title: "Lab 3 Matrix Algebra and Ordination Part I"
output:
  word_document: default
  pdf_document: default
  html_document:
    number_sections: yes
---

```{r setup, include=FALSE}
library(knitr)
opts_knit$set(root.dir = normalizePath("../"))
opts_chunk$set(echo = TRUE, tidy = TRUE)
```

# Set up R session


## Download packages 

Install and load packages

```{r warning=FALSE, message=FALSE}
library(MVA)
library(MVN)
library(psych)
library(Hmisc)
library(vegan)
library(StatMatch)
library(MASS)
library(dplyr)
```


# A primer of matrix algebra

Let’s start by making our own matrix:

```{r}
newMatrix<- matrix(c(1,4,5,4,5,6,9,1,9),nrow=3, ncol=3)
newMatrix
```
The command `c` concatenates a list of numbers.

Now let’s check the dimensions of newMatrix:

```{r}
dim(newMatrix)
```

## Matrix addition and subtraction

## **Question 1:** Create a new matrix to either add to or subtract from "newMatrix." This new matrix should be a 3 x 3 matrix containing all ones and call it oneMatrix. (20 pts)
```{r}
oneMatrix <- matrix(c(5,7,3,5,4,6,8,2,1), nrow = 3, ncol = 3)
oneMatrix
```


Now add oneMatrix to newMatrix:

```{r eval=FALSE}
newMatrix + oneMatrix #each cell in newM is added to the corresponding cell in oneM
```



Then subtract oneMatrix from newMatrix:
```{r eval=FALSE}
newMatrix - oneMatrix
```



**Remember, because matrix addition and subtraction is performed on an element by element basis, matrices must have the same dimensions.**

## Scalar Multiplication

A **Scalar** is a single number. Scalar multiplication multiplies a scalar times a matrix:

```{r eval=FALSE}
3*newMatrix
```


An *eigenvalue* is a scalar that is an essential component of multivariate analysis. We will explore this in a little bit.


## Matrix Multiplication
You use `%` to signify that are using a matrix operation. Otherwise, R will attempt the operation element by element.

```{r eval=FALSE}
oneMatrix%*%newMatrix
```

order matters:

```{r eval=FALSE}
newMatrix%*%oneMatrix
```



**The number of columns in the first matrix must equal the number of rows in the second matrix.**


## Matrix transposition

**Transposing** a matrix involves interchanging its rows and columns:

```{r}
transMatrix<-t(newMatrix)

```


## Identity Matrices

An **identity matrix** is a matrix where all the diagonal terms equal one and the remaining elements equal 0:

```{r}
Identity<-diag(3)

```
\newline

## Matrix Inversion

The inverse of matrix A is A-1. 


```{r}
invMatrix<-solve(newMatrix)

```


Multiplying a matrix by its inverse yields and identity matrix (A x A-1 = I):


```{r eval=FALSE}
invMatrix%*%newMatrix
```

Let’s round it:

```{r eval=FALSE}
round(invMatrix%*%newMatrix,10)
```

\newline

## Eigenvalues and eigenvectors

Remember that an eigenvalue is a special scalar and the associated eigenvector is a vector that are key components of PCA.

```{r}
eig<-eigen(newMatrix)
```

\newline


#	Principal Component Analysis (PCA)
You are going to conduct a PCA on the `USairpollution` data that we used in Lab 2.

First let’s call in the data:

```{r}
usAir<-USairpollution
```


Now, look at the distributions (i.e., histograms) of the variables to determine if they need to be transformed. You should be able to make the histograms and transform them based on what you learned during Lab 2. 
```{r}
par(mfrow=c(2, 4))
hist(usAir$SO2, main="raw SO2", xlab="")
hist(usAir$temp, main="raw temp", xlab="")
hist(usAir$manu, main="raw manu", xlab="")
hist(usAir$popul, main="raw popul", xlab="")
hist(usAir$wind, main="raw wind", xlab="")
hist(usAir$precip, main="raw precip", xlab="")
hist(usAir$predays, main="raw predays", xlab="")
```

```{r, testing normality}
mvn(usAir, mvnTest = "mardia")
```
Wind & predays are normal - do not need transformation

```{r}
logusAir = log(usAir)

par(mfrow=c(2, 4))
hist(logusAir$SO2, main="log SO2", xlab="") # *transformation works
hist(logusAir$temp, main="log temp", xlab="") # *
hist(logusAir$manu, main="log manu", xlab="") # *
hist(logusAir$popul, main="log popul", xlab="") # *
hist(logusAir$wind, main="log wind", xlab="") # does not need transformation
hist(logusAir$precip, main="log precip", xlab="") # does not need transformation
hist(logusAir$predays, main="log predays", xlab="") # does not need transformation
```

```{r, testing normality}
mvn(logusAir, mvnTest = "mardia")
```

Everything normal except precip & predays (does not need transformation anyway), and log not appropriate for popul and manu (count data). Log for precip improves normality than if it were to be left untransformed

```{r}
sqrtusAir = sqrt(usAir)

par(mfrow=c(2, 4))
hist(sqrtusAir$SO2, main="sqrt SO2", xlab="") #log better
hist(sqrtusAir$temp, main="sqrt temp", xlab="") #log better
hist(sqrtusAir$manu, main="sqrt manu", xlab="") #log better but not appropriate for count
hist(sqrtusAir$popul, main="sqrt popul", xlab="") #log better but not appropriate for count
hist(sqrtusAir$wind, main="sqrt wind", xlab="") # does not need transformation - period
hist(sqrtusAir$precip, main="sqrt precip", xlab="") # does not need transformation - not that much diff from raw & log is worse
hist(sqrtusAir$predays, main="sqrt predays", xlab="") # does not need transformation
```

```{r, testing normality}
mvn(sqrtusAir, mvnTest = "mardia")
```

Sqrt improves the normality of manu and popul and appropriate transformation for the data class. Precip seems to be more normal raw than transformed. 

```{r}
transusAir = usAir %>%
  mutate(logSO2 = log(SO2), logtemp = log(temp), sqrtmanu = sqrt(manu), sqrtpop = sqrt(popul)) %>%
  select(-c("SO2", "temp", "manu", "popul"))

head(transusAir, 4)
```

```{r, testing normality}
mvn(transusAir, mvnTest = "mardia")
```

```{r}
par(mfrow=c(2,4))

hist(transusAir$logSO2, main="log SO2", xlab="") 
hist(transusAir$logtemp, main="log temp", xlab="") 
hist(transusAir$sqrtmanu, main="sqrt manu", xlab="") 
hist(transusAir$sqrtpop, main="sqrt popul", xlab="") 
hist(transusAir$wind, main="raw wind", xlab="")
hist(transusAir$precip, main="raw precip", xlab="")
hist(transusAir$predays, main="raw predays", xlab="")
```


## **What variable did you transform, and what transformation did you use? (15 pts)**
I log transformed `SO2` and `temp`, and square root transformed `manu` and `popul`.  The rest were left as is, log and square root transformations exaggerated skewness. 

If you do transform any variables, use the transformed data matrix going forward.

Next, apply a z-score standardization:

```{r results='hide'}
ZusAir<-scale(transusAir) #especially important to scale bc in very different units now 
```
\newline

# Running the PCA:
You are going to use the package `princomp` function in the stats package. Take some time to read about this function:


```{r eval=FALSE}
?princomp
```

Run the princomp function:

```{r}
usAir_pca <- princomp(ZusAir, cor = F)
```

cor = F, because you are using the *covariance* matrix instead of the *correlation* matrix.

**It should be noted that the covariance matrix of a z-standardized data matrix is equivalent to the correlation matrix of the unscaled data.**

\newline

Let’s look at a summary of our PCA:

```{r}
summary(usAir_pca)
```

Notice that the summary gives the standard deviation instead of the eigenvalue (variance). Let’s calculate the eigenvalues using what we know about the relationship between standard deviation and variance (var = sd^2):

```{r}
eigenVal<- (usAir_pca$sdev*sqrt(41/40))^2
```



The *sqrt(41/40)* is to correct for the fact that princomp calculates variances with the divisor N instead of N-1 as is customary. This adjustment will allow direct comparison with “hand” calculated eigenvalues using the function `eigen` below. Note that these hand calculations are just to help you understand what is going on 'under the hood' in the model. 

Let’s make the PCA table with the eigenvalues instead of the standard deviations:

```{r}
propVar<-eigenVal/sum(eigenVal) #recalculating column to include eigenvalue
cumVar<-cumsum(propVar) #recalculating column to include eigenvalue
pca_Table<-t(rbind(eigenVal,propVar,cumVar))
pca_Table
```


**This calculation and table are just to show you that the eigenvalues and the output from `princomp`, the stadard deviations, are the same thing. YOU DO NOT NEED TO DO THIS WHEN YOU RUN A PCA - it is just to help you gain a deeper understanding of the model.**


the factor loadings from `princomp`:

```{r}
loadings(usAir_pca)
```


and the factor scores:

```{r eval=FALSE}
scores(usAir_pca)
```




\newline

## Determining number of axes to keep

You now have 7 PC axes. Which ones give us vital information and which ones can we toss? One method for selecting the number of Axes is a **Scree plot**:

```{r}
plot(usAir_pca, type="lines")
```
**_**
Based on the scree plot, it seems like we can keep the first five since they explain the majoritiy of the variation and the variation seems to level off around component 5. 


How about the **latent root criterion (i.e., keep components with eigenvalues > 1)** and the **relative percent variance criteria**.  Check the summary of the PCA explore this: 

```{r}
summary(usAir_pca)
```


## **Question 2: How many axes you should keep and why? (20 points)**
Based on the latent root criterion, we should keep the first three components since after that eigenvalues are < 1. 
\newline

## Significance of factor loadings.
While many use the “rule of thumb” where a loading > 0.30 dictates an “important” variable. Another method for determining significance of factor loadings is bootstrapping. Details and comparisons of the many way to assess significance of factor loadings are presented in Peres-Neto et al. (2003), which is supplemental reading for this week. Here we will run the method that they found to have the lowest type I error rates, Bootstrapped eigenvector. For reference, this is the method 6 in Peres-Neto et al. (2003).

```{r}
sigpca2<-function (x, permutations=1000, ...)
{
  pcnull <- princomp(x, ...)
  res <- pcnull$loadings
  out <- matrix(0, nrow=nrow(res), ncol=ncol(res))
  N <- nrow(x)
  for (i in 1:permutations) { #resample from x, princomp() it and compare PC info to orig PC, do this 1k times
    pc <- princomp(x[sample(N, replace=TRUE), ], ...) #randomly sample (w/ replacment) from the observed data pool (x), with the same number of rows (N) and no change to the columns
    pred <- predict(pc, newdata = x)
    r <-  cor(pcnull$scores, pred)
    k <- apply(abs(r), 2, which.max)
    reve <- sign(diag(r[k,]))
    sol <- pc$loadings[ ,k]
    sol <- sweep(sol, 2, reve, "*")
    out <- out + ifelse(res > 0, sol <=  0, sol >= 0)
  }
  out/permutations
}

set.seed(4) #so we get the same random sample 
 
sigpca2(ZusAir, permutations=1000) # Use function with the scaled data as input



# THE FOLLOWING IS INCLUDED IN THE FUNCTION BUT FOR ONE ITERATION
#Piece-by-piece (read along step-by-step with pg. 2350 section 6) Bootstrapped eigenvector (V vectors)” of Peres-Neto et al. 2003.
pcnull<-princomp(ZusAir) #
res <- pcnull$loadings
out <- matrix(0, nrow=nrow(res), ncol=ncol(res))
N <- nrow(ZusAir)
pc<-princomp(ZusAir[sample(N, replace=TRUE), ])
pred <- predict(pc, newdata = ZusAir)        
r <-  cor(pcnull$scores, pred)
k <- apply(abs(r), 2, which.max)
reve <- sign(diag(r[k,]))
sol <- pc$loadings[ ,k]
sol <- sweep(sol, 2, reve, "*")
out <- out + ifelse(res > 0, sol <=  0, sol >= 0)
```


\newline

## PCA plots
Plot out the factor loadings for the first 2 PC axes:

```{r fig.height= 7, fig.width= 7}
plot(usAir_pca$loadings, type="n", xlab="PC 1, 39%", ylab="PC 2, 21%", ylim=c(-.8,.8), xlim=c(-.6,.6))
text(usAir_pca$loadings, labels=as.character(colnames(ZusAir)), pos=1, cex=1)
```


## **Question 3: How do you interpret these axes? Come up with a name for each. (20 pts)**
The axes indicate how strongly each characteristic (variable vector) influence the variation (principal components). PC 1 could be deemed the "anthropogenic" axis where the majority of the variation is explained by the population (sqrtpopul), manufacturing enterprises (sqrtmanu) and the $SO_2$ air content (logSO2), which are all direct anthropogenic activities. PC 1 can be deemed the "temperature" axis since only temperature (logtemp) is really dominating that axis. 

**Close the plot window after viewing**

\pagebreak

Let’s now plot the PC score for each sample (city):

```{r fig.height= 7, fig.width= 7 }
plot(usAir_pca$scores,type="n",xlab="PC 1, 39%", ylab="PC 2, 21%",ylim=c(-2.5,4), xlim=c(-4,8))
text(usAir_pca$scores, labels=as.character(rownames(ZusAir)), pos=1, cex=1)
```


\newline

And now all together in a `biplot`:

```{r eval=FALSE}
?biplot
```

```{r fig.height= 7, fig.width= 7}
biplot(usAir_pca$scores, usAir_pca$loading, xlab="PC 1, 39%", ylab="PC 2, 21%", expand= 4, ylim=c(-6.5,6), xlim=c(-4,7.5))
```


\newline

to replace city names with a symbol:

```{r fig.height= 7, fig.width= 7}
biplot(usAir_pca$scores, usAir_pca$loading, expand= 4, xlabs= rep("*",41), xlab="PC 1, 39%", ylab="PC 2, 21%", ylim=c(-6.5,6), xlim=c(-4,7))

```

\newline

# Eigen Analysis

You can also just simply use the Eigen analysis function, `eigen` and calculate your own scores by hand. Note that I am showing this for illustration for those of you who want to have a deeper understanding of the method.

```{r}
eig<-eigen(cov(ZusAir))
```

Extract the first two eigenvectors (because that is what we are interested in plotting):
```{r}
eigVec<-as.matrix(eig$vectors[,1:2])
rownames(eigVec) <- rownames(cov(ZusAir))
```

Then simply multiply each eigenvector times the matrix of standardized observation values (ZusAir) and plot!

```{r fig.height= 7, fig.width= 7}
scores<-t(rbind(eigVec[,1]%*%t(ZusAir),eigVec[,2]%*%t(ZusAir)))#hand calculated scores

# and plot

biplot(scores,eigVec,xlab="PC 1, 39%", ylab="PC 2, 21%",expand= 4, ylim=c(-6.5,6), xlim=c(-4,7.5))
```


# My Data set 
## **Question 4: Does your individual dataset (the one used in Lab 2) meet the assumptions of PCA? Is PCA an analysis you could use on your data? (40 pts)** 
My data set does not meet the assumptions of multivariate normality (even after transformations), and so a PCA is not appropriate for this data set.  

